---
title: "Project 4"
author: "Gajendra Gurung & Rohan Ruthvik Kendyala"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r}
# Load necessary libraries
library(readr)
library(dplyr)
library(caret)
library(randomForest)
library(gbm)
library(xgboost)

# Read the data
housing_data <- read_csv("Housing.csv")

# Convert categorical variables to factors
categorical_features <- c("mainroad", "guestroom", "basement", "hotwaterheating", "airconditioning", "prefarea", "furnishingstatus")
housing_data[categorical_features] <- lapply(housing_data[categorical_features], as.factor)

# Split the data
set.seed(42)
training_rows <- createDataPartition(housing_data$price, p=0.7, list=FALSE)
train_data <- housing_data[training_rows, ]
test_data <- housing_data[-training_rows, ]

# Linear Regression
linear_model <- lm(price ~ ., data = train_data)
summary(linear_model)

# Decision Tree
library(rpart)
tree_model <- rpart(price ~ ., data = train_data)
summary(tree_model)

# Random Forest
rf_model <- randomForest(price ~ ., data = train_data, ntree=100)
summary(rf_model)

# Gradient Boosting Machine
gbm_model <- gbm(price ~ ., data = train_data, distribution = "gaussian", 
                 n.trees = 100, interaction.depth = 3, shrinkage = 0.1, cv.folds = 5)
summary(gbm_model)

# Predict and evaluate Linear Regression
predictions_lm <- predict(linear_model, test_data)
mse_lm <- mean((predictions_lm - test_data$price)^2)
rmse_lm <- sqrt(mse_lm)

# Predict and evaluate Decision Tree
predictions_tree <- predict(tree_model, test_data)
mse_tree <- mean((predictions_tree - test_data$price)^2)
rmse_tree <- sqrt(mse_tree)

# Predict and evaluate Random Forest
predictions_rf <- predict(rf_model, test_data)
mse_rf <- mean((predictions_rf - test_data$price)^2)
rmse_rf <- sqrt(mse_rf)

# Predict and evaluate GBM
predictions_gbm <- predict(gbm_model, test_data, n.trees = 100)
mse_gbm <- mean((predictions_gbm - test_data$price)^2)
rmse_gbm <- sqrt(mse_gbm)

# Print results
print(c("RMSE Linear Regression" = rmse_lm, "RMSE Decision Tree" = rmse_tree, "RMSE Random Forest" = rmse_rf, "RMSE GBM" = rmse_gbm))
```

```{r}
# Plot for Linear Regression
# Generate predictions
predictions_lm <- predict(linear_model, test_data)

# Calculate residuals
residuals_lm <- test_data$price - predictions_lm

# Plot residuals vs fitted values
plot(predictions_lm, residuals_lm, main="Residuals vs Fitted for Linear Regression", xlab="Fitted Values", ylab="Residuals")
abline(h=0, col="red")  # Add a horizontal line at zero


# Data preparation
rmse_data <- data.frame(
  Model = c("Linear Regression", "Decision Tree", "Random Forest", "GBM"),
  RMSE = c(rmse_lm, rmse_tree, rmse_rf, rmse_gbm)
)


ggplot(rmse_data, aes(x = Model, y = RMSE, fill = Model)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  theme_minimal() +
  labs(title = "RMSE Comparison Among Models", x = NULL, y = "RMSE")

# You can create similar plots for other models if they support residuals

# Feature importance for Random Forest
importance_rf <- randomForest::importance(rf_model)
importance_data_rf <- data.frame(Feature = rownames(importance_rf), Importance = importance_rf[,1])

ggplot(importance_data_rf, aes(x = reorder(Feature, -Importance), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +  # Flips the axes for easier reading
  labs(title = "Feature Importance for Random Forest", x = "Feature", y = "Importance")


```